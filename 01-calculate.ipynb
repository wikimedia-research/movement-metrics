{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata.utils import print_err, pd_display_all\n",
    "\n",
    "import src.content as content\n",
    "\n",
    "from src.utils import load_metric_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the date parameters are determined from these two:\n",
    "# * metrics_month_text (e.g. \"2023-08\"): the month metrics are generated for\n",
    "# * mediawiki_history_snapshot (e.g. \"2023-08\"): the version of mediawiki_history\n",
    "#   used to generate the editing metrics. This should generally be the latest available,\n",
    "#   even if you are not generating metrics for the latest month.\n",
    "#\n",
    "# Both key parameters are generated automatically by assuming they are the last completed\n",
    "# month, but you can manually set them to different values if necessary.\n",
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "\n",
    "metrics_month_text = last_month.strftime(\"%Y-%m\")\n",
    "mediawiki_history_snapshot = metrics_month_text\n",
    "\n",
    "\n",
    "# Convert our two date parameters to all the formats we need and provide them in a dict\n",
    "# so we can easily use them to format strings\n",
    "metrics_month = pd.Period(metrics_month_text)\n",
    "\n",
    "date_params = {\n",
    "    \"api_metrics_month_first_day\": metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"api_metrics_month_day_after\": (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"mediawiki_history_snapshot\": mediawiki_history_snapshot,\n",
    "    \"metrics_cur_month\": metrics_month.month,\n",
    "    \"metrics_month\": str(metrics_month),\n",
    "    \"metrics_month_first_day\": str(metrics_month.asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_month_end\": str((metrics_month + 1).start_time),\n",
    "    \"metrics_month_last_day\": str(metrics_month.asfreq(\"D\", how=\"end\")),\n",
    "    \"metrics_month_start\": str(metrics_month.start_time),\n",
    "    \"metrics_next_month_first_day\": str((metrics_month + 1).asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_prev_month\": str(metrics_month - 1),\n",
    "    \"metrics_year\": metrics_month.year,\n",
    "    \"retention_cohort\": str(metrics_month - 2)\n",
    "}\n",
    "\n",
    "def prepare_query(filename):\n",
    "    return (\n",
    "        Path(filename)\n",
    "        .read_text()\n",
    "        .format(**date_params)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricSet:\n",
    "    \"\"\"\n",
    "    A MetricSet is a group of monthly metrics that is saved to a single file and\n",
    "    which is generated by one or more queries.\n",
    "    \n",
    "    Class assumptions:\n",
    "    * Each query contains a \"month\" column which Pandas can parse into a date.\n",
    "    * The column names used in the queries are unique across queries.\n",
    "    \n",
    "    A note on the date formats:\n",
    "    The month column is saved to the file as the ISO-8601 date of the start of the\n",
    "    month (e.g. 2020-06-01). However, in Python we handle them not as Datetimes but as\n",
    "    Periods, since this makes it easy to write code (e.g. utils.calc_rpt) which works\n",
    "    both with the normal monthly as well as quarterly aggregates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, queries):\n",
    "        self.filename = filename\n",
    "        self.queries = queries\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.data = load_metric_file(self.filename)\n",
    "        except FileNotFoundError:\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def add_data(self, new_data):\n",
    "        \"\"\"\n",
    "        Takes a Pandas data frame with a date index giving the month and one or more\n",
    "        columns of metrics.\n",
    "        \"\"\"\n",
    "        # Our policy is to avoid altering previously-generated data (which can happen\n",
    "        # because some of our data sources regenerate history every month). If you do want\n",
    "        # to regenerate some data, manually delete it from the data file.\n",
    "        \n",
    "        # Store the original order of columns from self.data\n",
    "        original_order = self.data.columns.tolist()\n",
    "     \n",
    "        self.data = self.data.combine_first(new_data)[original_order]\n",
    "        \n",
    "    def run_queries(self, cleanup_function=None):\n",
    "        for key, val in self.queries.items():\n",
    "            query = prepare_query(val[\"file\"])\n",
    "            print_err(f\"Running {key} \")\n",
    "\n",
    "            result = wmf.spark.run(query)\n",
    "\n",
    "            result = (\n",
    "                result\n",
    "                .assign(month=lambda df: pd.to_datetime(df[\"month\"]))\n",
    "                .set_index(\"month\")\n",
    "                # This dataframe will usually have only a single row, so\n",
    "                # Pandas will not be able to infer the frequency of the period\n",
    "                .to_period(\"M\")\n",
    "            \n",
    "               \n",
    "            )\n",
    "            \n",
    "            if cleanup_function:\n",
    "                result = cleanup_function(result)\n",
    "            self.add_data(result)\n",
    "\n",
    "    def save_data(self):\n",
    "        self.data.to_timestamp().to_csv(self.filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running active_editors \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running edits                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running new_editor_retention                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_edits_editors                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_new_editor_retention                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "editing_queries = {\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.sql\"\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.sql\"\n",
    "    },\n",
    "    \"mobile-heavy_edits_editors\": {\n",
    "        \"file\": \"queries/mobile-heavy_edits_editors.sql\"\n",
    "    },\n",
    "    \"mobile-heavy_new_editor_retention\": {\n",
    "        \"file\": \"queries/mobile-heavy_new_editor_retention.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_metrics = MetricSet(\"metrics/editing_metrics.tsv\", editing_queries)\n",
    "editing_metrics.run_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    \"https://wikimedia.org/api/rest_v1/metrics/\"\n",
    "    \"edited-pages/new/{project}/all-editor-types/{page_type}/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"https://github.com/wikimedia-research/movement-metrics (bot)\"\n",
    "}\n",
    "\n",
    "api_results = []\n",
    "\n",
    "def get_new_pages(\n",
    "    project=\"all-projects\",\n",
    "    page_type=\"content\",\n",
    "    start=date_params[\"api_metrics_month_first_day\"],\n",
    "    end=date_params[\"api_metrics_month_day_after\"]\n",
    "):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project=project,\n",
    "        page_type=page_type,\n",
    "        start=start,\n",
    "        end=end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    if r.status_code == 404:\n",
    "        return None  # Returns None if the status code is 404\n",
    "    else:\n",
    "        data = r.json()[\"items\"][0][\"results\"]\n",
    "        frame = pd.DataFrame(data)\n",
    "        frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"]).dt.tz_localize(None)\n",
    "        frame = (\n",
    "            frame\n",
    "            .rename(columns={\"timestamp\": \"month\"})\n",
    "            .set_index(\"month\")\n",
    "            .to_period(\"M\")\n",
    "        )\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new = (\n",
    "    get_new_pages()\n",
    "    .rename(columns={\"new_pages\": \"net_new_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(total_new)\n",
    "\n",
    "\n",
    "wikidata_new = (\n",
    "    get_new_pages(project=\"wikidata.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Wikidata_entities\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(wikidata_new)\n",
    "\n",
    "\n",
    "commons_new = (\n",
    "    get_new_pages(project=\"commons.wikimedia.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Commons_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(commons_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on project 50 of 336 (ceb.wikipedia.org)\n",
      "Now on project 100 of 336 (gd.wikipedia.org)\n",
      "Now on project 150 of 336 (ki.wikipedia.org)\n",
      "Now on project 200 of 336 (mt.wikipedia.org)\n",
      "Now on project 250 of 336 (roa-rup.wikipedia.org)\n",
      "Now on project 300 of 336 (to.wikipedia.org)\n"
     ]
    }
   ],
   "source": [
    "wp_domains = wmf.spark.run(\"\"\"\n",
    "    SELECT domain_name\n",
    "    FROM canonical_data.wikis\n",
    "    WHERE database_group = \"wikipedia\"\n",
    "\"\"\")[\"domain_name\"]\n",
    "\n",
    "results = []\n",
    "n = len(wp_domains)\n",
    "\n",
    "for i, domain in enumerate(wp_domains):\n",
    "    p = i + 1\n",
    "    if p % 50 == 0:\n",
    "        print(f\"Now on project {p} of {n} ({domain})\")\n",
    "\n",
    "    frame = get_new_pages(project=domain)\n",
    "    \n",
    "    if frame is not None:\n",
    "        frame[\"project\"] = domain\n",
    "        results.append(frame)\n",
    "    \n",
    "    # Be polite to the API\n",
    "    time.sleep(0.02)\n",
    "\n",
    "if results:\n",
    "    new_per_wp = pd.concat(results)\n",
    "\n",
    "    # Sum across projects to get new Wikipedia articles per month\n",
    "    wikipedia_new = (\n",
    "        new_per_wp\n",
    "        .groupby(\"month\")\n",
    "        .agg({\"new_pages\": \"sum\"})\n",
    "        .rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"})\n",
    "    )\n",
    "\n",
    "    editing_metrics.add_data(wikipedia_new)\n",
    "else:\n",
    "    print(\"No data to aggregate.\")\n",
    "\n",
    "editing_metrics.add_data(wikipedia_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readers metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running pageviews \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running automated_pageviews                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running page_previews                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running unique_devices                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "readers_queries = {\n",
    "    \"pageviews\": {\n",
    "        \"file\": \"queries/pageviews.sql\"\n",
    "    },\n",
    "    \"automated_pageviews\": {\n",
    "        \"file\": \"queries/automated_pageviews.sql\"\n",
    "    },\n",
    "    \"page_previews\": {\n",
    "        \"file\": \"queries/page_previews.sql\"\n",
    "    },\n",
    "    \"unique_devices\": {\n",
    "        \"file\": \"queries/unique_devices.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "readers_metrics = MetricSet(\"metrics/readers_metrics.tsv\", readers_queries)\n",
    "readers_metrics.run_queries()\n",
    "readers_metrics.data = (\n",
    "    readers_metrics.data\n",
    "    .assign(interactions=lambda df: df[\"previews_seen\"] + df[\"total_pageview\"])\n",
    ")\n",
    "readers_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running regional_unique_devices \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regional_unique_devices_queries = {\n",
    "    \"regional_unique_devices\": {\n",
    "        \"file\": \"queries/regional_unique_devices.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "regional_unique_devices = MetricSet(\"metrics/regional_unique_devices.tsv\", regional_unique_devices_queries)\n",
    "regional_unique_devices.run_queries()\n",
    "regional_unique_devices.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing diversity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_edits_editors \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_new_editor_retention                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_net_new_content \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_wikidata_entities                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_edits_editors                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_new_editor_retention                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_net_new_content \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_wikidata_entities                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "editing_diversity_queries = {\n",
    "    \"global_south_edits_editors\": {\n",
    "        \"file\": \"queries/global_south_edits_editors.sql\"\n",
    "    },\n",
    "    \"global_south_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_south_new_editor_retention.sql\"\n",
    "    },\n",
    "    \"global_south_net_new_content\": {\n",
    "        \"file\": \"queries/global_south_net_new_content.sql\"\n",
    "    },\n",
    "    \"global_south_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_south_net_new_wikidata.sql\"\n",
    "    },\n",
    "    \"global_north_edits_editors\": {\n",
    "        \"file\": \"queries/global_north_edits_editors.sql\"\n",
    "    },\n",
    "    \"global_north_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_north_new_editor_retention.sql\"\n",
    "    },\n",
    "    \"global_north_net_new_content\": {\n",
    "        \"file\": \"queries/global_north_net_new_content.sql\"\n",
    "    },\n",
    "    \"global_north_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_north_net_new_wikidata.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_diversity_metrics = MetricSet(\"metrics/editing_diversity_metrics.tsv\", editing_diversity_queries)\n",
    "editing_diversity_metrics.run_queries()\n",
    "editing_diversity_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_pageviews \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_previews                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_previews                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_pageviews                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "readers_diversity_queries = {\n",
    "    \"global_south_pageviews\": {\n",
    "        \"file\": \"queries/global_south_pageviews.sql\"\n",
    "    },\n",
    "    \"global_south_previews\": {\n",
    "        \"file\": \"queries/global_south_previews.sql\"\n",
    "    },\n",
    "    \"global_north_previews\": {\n",
    "        \"file\": \"queries/global_north_previews.sql\"\n",
    "    },\n",
    "     \"global_north_pageviews\": {\n",
    "        \"file\": \"queries/global_north_pageviews.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "readers_diversity_metrics = MetricSet(\"metrics/readers_diversity_metrics.tsv\", readers_diversity_queries)\n",
    "readers_diversity_metrics.run_queries()\n",
    "\n",
    "readers_diversity_metrics.data = (\n",
    "    readers_diversity_metrics.data\n",
    "    .assign(\n",
    "        gs_interactions=lambda df: df[\"gs_previews\"] + df[\"gs_pageviews\"],\n",
    "        gn_interactions=lambda df: df[\"gn_previews\"] + df[\"gn_pageviews\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "readers_diversity_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content gap metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running content_gap \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(content)\n",
    "\n",
    "#     \n",
    "#    The content gap metrics are fetched from the csv dumps in the links presented in data_dict.  \n",
    "#    The dumps are updated around the 23rd of every month and since historical data must remain unchanged, the new data fetched from the \n",
    "#    dumps are appended to the tsv's which contain the historical snapshot.  Every new snapshot contains partial data of the month it is published in\n",
    "#    so the data from the the last month in the dataset is dropped in the clean up functions.\n",
    "   \n",
    "\n",
    "content_gap_queries = {\n",
    "    \"content_gap\": {\n",
    "        \"file\": \"queries/content_gap.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "content_gap_metrics = MetricSet(\"metrics/content_gap_data_metrics.tsv\", content_gap_queries)\n",
    "content_gap_metrics.run_queries(cleanup_function=content.process_quality_data)\n",
    "\n",
    "content_gap_metrics.data = content.calculate_mom(content_gap_metrics.data)\n",
    "content_gap_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
