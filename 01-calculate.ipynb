{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata.utils import print_err, pd_display_all\n",
    "\n",
    "import src.content as content\n",
    "\n",
    "from src.utils import load_metric_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the date parameters are determined from these two:\n",
    "# * metrics_month_text (e.g. \"2023-08\"): the month metrics are generated for\n",
    "# * mediawiki_history_snapshot (e.g. \"2023-08\"): the version of mediawiki_history\n",
    "#   used to generate the editing metrics. This should generally be the latest available,\n",
    "#   even if you are not generating metrics for the latest month.\n",
    "#\n",
    "# Both key parameters are generated automatically by assuming they are the last completed\n",
    "# month, but you can manually set them to different values if necessary.\n",
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "\n",
    "metrics_month_text = last_month.strftime(\"%Y-%m\")\n",
    "mediawiki_history_snapshot = metrics_month_text\n",
    "\n",
    "\n",
    "# Convert our two date parameters to all the formats we need and provide them in a dict\n",
    "# so we can easily use them to format strings\n",
    "metrics_month = pd.Period(metrics_month_text)\n",
    "\n",
    "date_params = {\n",
    "    \"api_metrics_month_first_day\": metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"api_metrics_month_day_after\": (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"mediawiki_history_snapshot\": mediawiki_history_snapshot,\n",
    "    \"metrics_cur_month\": metrics_month.month,\n",
    "    \"metrics_month\": str(metrics_month),\n",
    "    \"metrics_month_first_day\": str(metrics_month.asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_month_end\": str((metrics_month + 1).start_time),\n",
    "    \"metrics_month_last_day\": str(metrics_month.asfreq(\"D\", how=\"end\")),\n",
    "    \"metrics_month_start\": str(metrics_month.start_time),\n",
    "    \"metrics_next_month_first_day\": str((metrics_month + 1).asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_prev_month\": str(metrics_month - 1),\n",
    "    \"metrics_year\": metrics_month.year,\n",
    "    \"retention_cohort\": str(metrics_month - 2)\n",
    "}\n",
    "\n",
    "def prepare_query(filename):\n",
    "    return (\n",
    "        Path(filename)\n",
    "        .read_text()\n",
    "        .format(**date_params)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricSet:\n",
    "    \"\"\"\n",
    "    A MetricSet is a group of monthly metrics that is saved to a single file and\n",
    "    which is generated by one or more queries.\n",
    "    \n",
    "    Class assumptions:\n",
    "    * Each query contains a \"month\" column which Pandas can parse into a date.\n",
    "    * The column names used in the queries are unique across queries.\n",
    "    \n",
    "    A note on the date formats:\n",
    "    The month column is saved to the file as the ISO-8601 date of the start of the\n",
    "    month (e.g. 2020-06-01). However, in Python we handle them not as Datetimes but as\n",
    "    Periods, since this makes it easy to write code (e.g. utils.calc_rpt) which works\n",
    "    both with the normal monthly as well as quarterly aggregates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, queries):\n",
    "        self.filename = filename\n",
    "        self.queries = queries\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.data = load_metric_file(self.filename)\n",
    "        except FileNotFoundError:\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def add_data(self, new_data):\n",
    "        \"\"\"\n",
    "        Takes a Pandas data frame with a date index giving the month and one or more\n",
    "        columns of metrics.\n",
    "        \"\"\"\n",
    "        # Our policy is to avoid altering previously-generated data (which can happen\n",
    "        # because some of our data sources regenerate history every month). If you do want\n",
    "        # to regenerate some data, manually delete it from the data file.\n",
    "        \n",
    "        # Store the original order of columns from self.data\n",
    "        original_order = self.data.columns.tolist()\n",
    "     \n",
    "        self.data = self.data.combine_first(new_data)[original_order]\n",
    "        \n",
    "    def run_queries(self, cleanup_function=None):\n",
    "        for key, val in self.queries.items():\n",
    "            query = prepare_query(val[\"file\"])\n",
    "            print_err(f\"Running {key} \")\n",
    "\n",
    "            result = wmf.spark.run(query)\n",
    "\n",
    "            result = (\n",
    "                result\n",
    "                .assign(month=lambda df: pd.to_datetime(df[\"month\"]))\n",
    "                .set_index(\"month\")\n",
    "                # This dataframe will usually have only a single row, so\n",
    "                # Pandas will not be able to infer the frequency of the period\n",
    "                .to_period(\"M\")\n",
    "            \n",
    "               \n",
    "            )\n",
    "            \n",
    "            if cleanup_function:\n",
    "                result = cleanup_function(result)\n",
    "            self.add_data(result)\n",
    "\n",
    "    def save_data(self):\n",
    "        self.data.to_timestamp().to_csv(self.filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running active_editors \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/20 14:04:21 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/12/20 14:04:22 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/12/20 14:04:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/20 14:04:34 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/12/20 14:04:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/12/20 14:04:53 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "Running edits                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/20 14:05:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "Running new_editor_retention                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_edits_editors                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_new_editor_retention                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "editing_queries = {\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.sql\"\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.sql\"\n",
    "    },\n",
    "    \"mobile-heavy_edits_editors\": {\n",
    "        \"file\": \"queries/mobile-heavy_edits_editors.sql\"\n",
    "    },\n",
    "    \"mobile-heavy_new_editor_retention\": {\n",
    "        \"file\": \"queries/mobile-heavy_new_editor_retention.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_metrics = MetricSet(\"metrics/editing_metrics.tsv\", editing_queries)\n",
    "editing_metrics.run_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    \"https://wikimedia.org/api/rest_v1/metrics/\"\n",
    "    \"edited-pages/new/{project}/all-editor-types/{page_type}/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"https://github.com/wikimedia-research/movement-metrics (bot)\"\n",
    "}\n",
    "\n",
    "api_results = []\n",
    "\n",
    "def get_new_pages(\n",
    "    project=\"all-projects\",\n",
    "    page_type=\"content\",\n",
    "    start=date_params[\"api_metrics_month_first_day\"],\n",
    "    end=date_params[\"api_metrics_month_day_after\"]\n",
    "):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project=project,\n",
    "        page_type=page_type,\n",
    "        start=start,\n",
    "        end=end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    if r.status_code == 404:\n",
    "        return None  # Returns None if the status code is 404\n",
    "    else:\n",
    "        data = r.json()[\"items\"][0][\"results\"]\n",
    "        frame = pd.DataFrame(data)\n",
    "        frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"]).dt.tz_localize(None)\n",
    "        frame = (\n",
    "            frame\n",
    "            .rename(columns={\"timestamp\": \"month\"})\n",
    "            .set_index(\"month\")\n",
    "            .to_period(\"M\")\n",
    "        )\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new = (\n",
    "    get_new_pages()\n",
    "    .rename(columns={\"new_pages\": \"net_new_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(total_new)\n",
    "\n",
    "\n",
    "wikidata_new = (\n",
    "    get_new_pages(project=\"wikidata.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Wikidata_entities\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(wikidata_new)\n",
    "\n",
    "\n",
    "commons_new = (\n",
    "    get_new_pages(project=\"commons.wikimedia.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Commons_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(commons_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on project 50 of 339 (cdo.wikipedia.org)\n",
      "Now on project 100 of 339 (ga.wikipedia.org)\n",
      "Now on project 150 of 339 (kcg.wikipedia.org)\n",
      "Now on project 200 of 339 (mr.wikipedia.org)\n",
      "Now on project 250 of 339 (rmy.wikipedia.org)\n",
      "Now on project 300 of 339 (tly.wikipedia.org)\n"
     ]
    }
   ],
   "source": [
    "wp_domains = wmf.spark.run(\"\"\"\n",
    "    SELECT domain_name\n",
    "    FROM canonical_data.wikis\n",
    "    WHERE database_group = \"wikipedia\"\n",
    "\"\"\")[\"domain_name\"]\n",
    "\n",
    "results = []\n",
    "n = len(wp_domains)\n",
    "\n",
    "for i, domain in enumerate(wp_domains):\n",
    "    p = i + 1\n",
    "    if p % 50 == 0:\n",
    "        print(f\"Now on project {p} of {n} ({domain})\")\n",
    "\n",
    "    frame = get_new_pages(project=domain)\n",
    "    \n",
    "    if frame is not None:\n",
    "        frame[\"project\"] = domain\n",
    "        results.append(frame)\n",
    "    \n",
    "    # Be polite to the API\n",
    "    time.sleep(0.02)\n",
    "\n",
    "if results:\n",
    "    new_per_wp = pd.concat(results)\n",
    "\n",
    "    # Sum across projects to get new Wikipedia articles per month\n",
    "    wikipedia_new = (\n",
    "        new_per_wp\n",
    "        .groupby(\"month\")\n",
    "        .agg({\"new_pages\": \"sum\"})\n",
    "        .rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"})\n",
    "    )\n",
    "\n",
    "    editing_metrics.add_data(wikipedia_new)\n",
    "else:\n",
    "    print(\"No data to aggregate.\")\n",
    "\n",
    "editing_metrics.add_data(wikipedia_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readers metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running pageviews \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running automated_pageviews                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running page_previews                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running unique_devices                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "readers_queries = {\n",
    "    \"pageviews\": {\n",
    "        \"file\": \"queries/pageviews.sql\"\n",
    "    },\n",
    "    \"automated_pageviews\": {\n",
    "        \"file\": \"queries/automated_pageviews.sql\"\n",
    "    },\n",
    "    \"page_previews\": {\n",
    "        \"file\": \"queries/page_previews.sql\"\n",
    "    },\n",
    "    \"unique_devices\": {\n",
    "        \"file\": \"queries/unique_devices.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "readers_metrics = MetricSet(\"metrics/readers_metrics.tsv\", readers_queries)\n",
    "readers_metrics.run_queries()\n",
    "readers_metrics.data = (\n",
    "    readers_metrics.data\n",
    "    .assign(interactions=lambda df: df[\"previews_seen\"] + df[\"total_pageview\"])\n",
    ")\n",
    "readers_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running regional_unique_devices \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regional_unique_devices_queries = {\n",
    "    \"regional_unique_devices\": {\n",
    "        \"file\": \"queries/regional_unique_devices.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "regional_unique_devices = MetricSet(\"metrics/regional_unique_devices.tsv\", regional_unique_devices_queries)\n",
    "regional_unique_devices.run_queries()\n",
    "regional_unique_devices.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing diversity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_edits_editors \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_new_editor_retention                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_net_new_content \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_wikidata_entities                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_edits_editors                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_new_editor_retention                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_net_new_content \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_wikidata_entities                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "editing_diversity_queries = {\n",
    "    \"global_south_edits_editors\": {\n",
    "        \"file\": \"queries/global_south_edits_editors.sql\"\n",
    "    },\n",
    "    \"global_south_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_south_new_editor_retention.sql\"\n",
    "    },\n",
    "    \"global_south_net_new_content\": {\n",
    "        \"file\": \"queries/global_south_net_new_content.sql\"\n",
    "    },\n",
    "    \"global_south_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_south_net_new_wikidata.sql\"\n",
    "    },\n",
    "    \"global_north_edits_editors\": {\n",
    "        \"file\": \"queries/global_north_edits_editors.sql\"\n",
    "    },\n",
    "    \"global_north_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_north_new_editor_retention.sql\"\n",
    "    },\n",
    "    \"global_north_net_new_content\": {\n",
    "        \"file\": \"queries/global_north_net_new_content.sql\"\n",
    "    },\n",
    "    \"global_north_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_north_net_new_wikidata.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_diversity_metrics = MetricSet(\"metrics/editing_diversity_metrics.tsv\", editing_diversity_queries)\n",
    "editing_diversity_metrics.run_queries()\n",
    "editing_diversity_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_pageviews \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_previews                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_previews                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_pageviews                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "readers_diversity_queries = {\n",
    "    \"global_south_pageviews\": {\n",
    "        \"file\": \"queries/global_south_pageviews.sql\"\n",
    "    },\n",
    "    \"global_south_previews\": {\n",
    "        \"file\": \"queries/global_south_previews.sql\"\n",
    "    },\n",
    "    \"global_north_previews\": {\n",
    "        \"file\": \"queries/global_north_previews.sql\"\n",
    "    },\n",
    "     \"global_north_pageviews\": {\n",
    "        \"file\": \"queries/global_north_pageviews.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "readers_diversity_metrics = MetricSet(\"metrics/readers_diversity_metrics.tsv\", readers_diversity_queries)\n",
    "readers_diversity_metrics.run_queries()\n",
    "\n",
    "readers_diversity_metrics.data = (\n",
    "    readers_diversity_metrics.data\n",
    "    .assign(\n",
    "        gs_interactions=lambda df: df[\"gs_previews\"] + df[\"gs_pageviews\"],\n",
    "        gn_interactions=lambda df: df[\"gn_previews\"] + df[\"gn_pageviews\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "readers_diversity_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content gap metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running content_gap \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Content gap data is released around the 23rd of every month for the previous month's data.\n",
    "\n",
    "content_gap_queries = {\n",
    "    \"content_gap\": {\n",
    "        \"file\": \"queries/content_gap.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "content_gap_metrics = MetricSet(\"metrics/content_gap_data_metrics.tsv\", content_gap_queries)\n",
    "content_gap_metrics.run_queries(cleanup_function=content.process_quality_data)\n",
    "\n",
    "content_gap_metrics.data = content.calculate_mom(content_gap_metrics.data)\n",
    "content_gap_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
